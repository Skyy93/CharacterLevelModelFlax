{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FlaxRNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBg4vh7k2PI_",
        "colab_type": "code",
        "outputId": "672ad7df-1fff-49fa-fa8b-710eeaf937c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install -q --upgrade https://storage.googleapis.com/jax-releases/`nvcc -V | sed -En \"s/.* release ([0-9]*)\\.([0-9]*),.*/cuda\\1\\2/p\"`/jaxlib-0.1.42-`python3 -V | sed -En \"s/Python ([0-9]*)\\.([0-9]*).*/cp\\1\\2/p\"`-none-linux_x86_64.whl jax\n",
        "!pip install -q git+https://github.com/google/flax.git@master"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for flax (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp2jGD3w3H8R",
        "colab_type": "code",
        "outputId": "709ef817-ab20-4cff-ce5e-1985c383aa44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from jax.lib import xla_bridge\n",
        "print(xla_bridge.get_backend().platform)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPyzI-i24Lgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import jax\n",
        "import flax\n",
        "from flax import jax_utils\n",
        "from flax import nn\n",
        "from flax import optim\n",
        "from flax.training import common_utils\n",
        "\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import collections\n",
        "import functools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS4-XXvN4nQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Data input pipeline written in tensorflow \"\"\"\n",
        "\n",
        "def vocab(path='tiny-shakespeare.txt'):\n",
        "    file = open(path, 'r')\n",
        "    data = file.read()\n",
        "    freq = collections.Counter(data).most_common()\n",
        "    vocab = dict()\n",
        "    reverse_vocab = dict()\n",
        "    count = 0\n",
        "    for i in freq:\n",
        "        vocab[count] = i[0]\n",
        "        reverse_vocab[i[0]] = count\n",
        "        count = count + 1\n",
        "    return vocab, reverse_vocab\n",
        "\n",
        "\n",
        "def get_text_dataset(text, reverse_vocab, mode, sequence_length=50, batch_size=32):\n",
        "    reverse_list = list()\n",
        "    for i in text:\n",
        "        reverse_list.append(reverse_vocab[i])\n",
        "    ds_seq = tf.data.Dataset.from_tensor_slices(tf.one_hot(reverse_list, depth=len(reverse_vocab)))\n",
        "    ds_seq = ds_seq.batch(sequence_length, drop_remainder=True)\n",
        "    ds = ds_seq.map(lambda x: chunk(x))\n",
        "    if mode is tf.estimator.ModeKeys.TRAIN:\n",
        "        ds.shuffle(len(text))\n",
        "    ds = ds.batch(batch_size, drop_remainder=True)\n",
        "    return ds\n",
        "\n",
        "def chunk(x):\n",
        "    return x[:-1], x[1:]    \n",
        "        \n",
        "def test_ds(vocab_size):\n",
        "    start_id = np.random.randint(low=0, high=vocab_size, size=(1,1))\n",
        "    test_ds = tf.data.Dataset.from_tensor_slices(tensors=tf.convert_to_tensor(start_id, dtype=tf.int64))\n",
        "    test_ds = test_ds.map(map_func=lambda x:tf.one_hot(x, depth=vocab_size))\n",
        "    return test_ds\n",
        "\n",
        "f = open('tiny-shakespeare.txt', 'r')\n",
        "text = f.read()\n",
        "f.close()\n",
        "\n",
        "vocab, reverse_vocab = vocab()\n",
        "params = {'batch_size': 32, 'seq_length' : 50, 'learning_rate' : 0.002, 'epochs_per_decay': 5, 'learning_rate_decay' : 0.97, 'vocab_length' : len(vocab)}\n",
        "params['step_decay'] = params['epochs_per_decay'] * int( int( len(text) / params['seq_length']) / params['batch_size'])\n",
        "ds = get_text_dataset(text=text, reverse_vocab=reverse_vocab, mode=tf.estimator.ModeKeys.TRAIN)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xNAKQJO4t4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" The Flax RNN impementation \"\"\"\n",
        "\n",
        "class RNN(flax.nn.Module):\n",
        "    \"\"\"LSTM\"\"\"\n",
        "    def apply(self, carry, inputs):\n",
        "        carry1, outputs = jax_utils.scan_in_dim(\n",
        "            nn.LSTMCell.partial(name='lstm1'), carry[0], inputs, axis=1)\n",
        "        carry2, outputs = jax_utils.scan_in_dim(\n",
        "            nn.LSTMCell.partial(name='lstm2'), carry[1], outputs, axis=1)\n",
        "        carry3, outputs = jax_utils.scan_in_dim(\n",
        "            nn.LSTMCell.partial(name='lstm3'), carry[2], outputs, axis=1)\n",
        "        x = nn.Dense(outputs, features=params['vocab_length'], name='dense')\n",
        "        return [carry1, carry2, carry3], x\n",
        "\n",
        "class charRNN(flax.nn.Module):\n",
        "    \"\"\"Char Generator\"\"\"\n",
        "    def apply(self, inputs, carry_pred=None, train=True):\n",
        "        batch_size = params['batch_size']\n",
        "        vocab_size = params['vocab_length']\n",
        "        hidden_size = 512\n",
        "        if train:\n",
        "            carry1 = nn.LSTMCell.initialize_carry(jax.random.PRNGKey(0), (batch_size,),hidden_size)\n",
        "            carry2 = nn.LSTMCell.initialize_carry(jax.random.PRNGKey(0), (batch_size,),hidden_size)\n",
        "            carry3 = nn.LSTMCell.initialize_carry(jax.random.PRNGKey(0), (batch_size,),hidden_size)\n",
        "            carry = [carry1, carry2, carry3]\n",
        "            _, x = RNN(carry, inputs)\n",
        "            return x\n",
        "        else:\n",
        "            carry, x = RNN(carry_pred, inputs)\n",
        "            return carry, x\n",
        "        \n",
        "\n",
        "\n",
        "@jax.vmap\n",
        "def cross_entropy_loss(logits, labels):\n",
        "      \"\"\"Returns cross-entropy loss.\"\"\"\n",
        "      return -jnp.mean(jnp.sum(nn.log_softmax(logits) * labels))\n",
        "\n",
        "@jax.vmap\n",
        "def acc(logits, labels):\n",
        "      \"\"\"Returns accuracy.\"\"\"\n",
        "      return jnp.argmax(logits, -1) == jnp.argmax(labels, -1)\n",
        "\n",
        "def compute_metrics(logits, labels):\n",
        "    \"\"\"Computes metrics and returns them.\"\"\"\n",
        "    loss = jnp.mean(cross_entropy_loss(logits, labels)) / params['batch_size']\n",
        "    \n",
        "    accuracy = jnp.mean( acc(logits, labels)\n",
        "        )\n",
        "    metrics = {\n",
        "        'loss': loss,\n",
        "        'accuracy': accuracy,\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def train_step(optimizer, batch):\n",
        "    \"\"\"Train one step.\"\"\"\n",
        "    \n",
        "    def loss_fn(model):\n",
        "        \"\"\"Compute cross-entropy loss and predict logits of the current batch\"\"\"\n",
        "\n",
        "        logits = model(batch[0])        \n",
        "        loss = jnp.mean(cross_entropy_loss(logits, batch[1])) / params['batch_size']\n",
        "        return loss, logits\n",
        "\n",
        "    def exponential_decay(steps):\n",
        "        \"\"\"Decrease the learning rate every 5 epochs\"\"\"\n",
        "        x_decay = (steps / params['step_decay']).astype('int32')\n",
        "        ret = params['learning_rate']* jax.lax.pow((params['learning_rate_decay']), x_decay.astype('float32'))\n",
        "        return jnp.asarray(ret, dtype=jnp.float32)\n",
        "\n",
        "    current_step = optimizer.state.step\n",
        "    new_lr = exponential_decay(current_step)\n",
        "    # calculate and apply the gradient \n",
        "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "    (_, logits), grad = grad_fn(optimizer.target)\n",
        "    new_optimizer = optimizer.apply_gradient(grad, learning_rate=new_lr)\n",
        "\n",
        "    metrics = compute_metrics(logits, batch[1])\n",
        "    metrics['learning_rate'] = new_lr\n",
        "    return new_optimizer, metrics\n",
        "\n",
        "@jax.jit\n",
        "def sample(inputs, optimizer):\n",
        "    next_inputs = inputs\n",
        "    output = []\n",
        "    batch_size = 1 \n",
        "    carry1 = nn.LSTMCell.initialize_carry(jax.random.PRNGKey(0), (batch_size,),512)\n",
        "    carry2 = nn.LSTMCell.initialize_carry(jax.random.PRNGKey(0), (batch_size,),512)\n",
        "    carry3 = nn.LSTMCell.initialize_carry(jax.random.PRNGKey(0), (batch_size,),512)\n",
        "    carry = [carry1, carry2, carry3]\n",
        "\n",
        "    def inference(model, carry):\n",
        "        carry, rnn_output = model(inputs=next_inputs, train=False, carry_pred=carry)\n",
        "        return carry, rnn_output\n",
        "  \n",
        "    for i in range(200):\n",
        "        carry, rnn_output = inference(optimizer.target, carry)\n",
        "        output.append(jnp.argmax(rnn_output, axis=-1))\n",
        "        # Select the argmax as the next input.\n",
        "        next_inputs = jnp.expand_dims(common_utils.onehot(jnp.argmax(rnn_output), params['vocab_length']), axis=0)\n",
        "    return output      \n",
        "\n",
        "\n",
        "def create_model(rng):\n",
        "    \"\"\"Creates a model.\"\"\"\n",
        "    vocab_size = params['vocab_length']\n",
        "    _, initial_params = charRNN.init_by_shape(\n",
        "        rng, [((1, params['seq_length'], vocab_size), jnp.float32)])\n",
        "    model = nn.Model(charRNN, initial_params)\n",
        "    return model\n",
        "\n",
        "def create_optimizer(model, learning_rate):\n",
        "    \"\"\"Creates an Adam optimizer for model.\"\"\"\n",
        "    optimizer_def = optim.Adam(learning_rate=learning_rate, weight_decay=1e-1)\n",
        "    optimizer = optimizer_def.create(model)\n",
        "    return optimizer  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KH_CD4hDjty",
        "colab_type": "code",
        "outputId": "904ed8b0-5f60-4813-a2a1-61a62f86e222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def train_model():\n",
        "    \"\"\"Train and inference \"\"\"\n",
        "    rng = jax.random.PRNGKey(0)\n",
        "    model = create_model(rng)\n",
        "    optimizer = create_optimizer(model, params['learning_rate'])\n",
        "\n",
        "    del model\n",
        "    for epoch in range(100):\n",
        "\n",
        "        for text in tfds.as_numpy(ds):\n",
        "            optimizer, metrics = train_step(optimizer, text)\n",
        "\n",
        "        print('epoch: %d, loss: %.4f, accuracy: %.2f, LR: %.8f' % (epoch+1,metrics['loss'], metrics['accuracy'] * 100, metrics['learning_rate']))\n",
        "        test = test_ds(params['vocab_length'])\n",
        "        sampled_text = \"\"\n",
        "\n",
        "        if ((epoch+1)%10 == 0):\n",
        "            for i in test:\n",
        "                sampled_text += vocab[int(jnp.argmax(i.numpy(),-1))]\n",
        "                start = np.expand_dims(i, axis=0)\n",
        "                text = sample(start, optimizer)\n",
        "\n",
        "            for i in text:\n",
        "                sampled_text += vocab[int(i)]\n",
        "            print(sampled_text)\n",
        "\n",
        "train_model()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, loss: 2.5571, accuracy: 50.51, LR: 0.00200000\n",
            "epoch: 2, loss: 2.2637, accuracy: 54.78, LR: 0.00200000\n",
            "epoch: 3, loss: 2.1183, accuracy: 57.84, LR: 0.00200000\n",
            "epoch: 4, loss: 2.0430, accuracy: 58.93, LR: 0.00200000\n",
            "epoch: 5, loss: 1.9602, accuracy: 60.52, LR: 0.00200000\n",
            "epoch: 6, loss: 1.8923, accuracy: 60.84, LR: 0.00194000\n",
            "epoch: 7, loss: 1.8473, accuracy: 62.44, LR: 0.00194000\n",
            "epoch: 8, loss: 1.8059, accuracy: 63.52, LR: 0.00194000\n",
            "epoch: 9, loss: 1.7579, accuracy: 64.67, LR: 0.00194000\n",
            "epoch: 10, loss: 1.7166, accuracy: 65.05, LR: 0.00194000\n",
            "peak the mariners all the merchant of the meaning of the meaning of the meaning of the meaning of the meaning of the meaning of the meaning of the meaning of the meaning of the meaning of the meaning o\n",
            "epoch: 11, loss: 1.6842, accuracy: 64.86, LR: 0.00188180\n",
            "epoch: 12, loss: 1.6492, accuracy: 64.67, LR: 0.00188180\n",
            "epoch: 13, loss: 1.6598, accuracy: 65.31, LR: 0.00188180\n",
            "epoch: 14, loss: 1.6123, accuracy: 66.58, LR: 0.00188180\n",
            "epoch: 15, loss: 1.5854, accuracy: 66.71, LR: 0.00188180\n",
            "epoch: 16, loss: 1.5449, accuracy: 67.98, LR: 0.00182535\n",
            "epoch: 17, loss: 1.5421, accuracy: 67.79, LR: 0.00182535\n",
            "epoch: 18, loss: 1.5357, accuracy: 68.43, LR: 0.00182535\n",
            "epoch: 19, loss: 1.4914, accuracy: 68.43, LR: 0.00182535\n",
            "epoch: 20, loss: 1.4842, accuracy: 69.07, LR: 0.00182535\n",
            "and the princess of his most soldier.\n",
            "\n",
            "ANTONIO:\n",
            "The first man that made great fooling of his mother.\n",
            "\n",
            "ANTONIO:\n",
            "The feether hand the fire of men.\n",
            "\n",
            "ANTONIO:\n",
            "The feether hands whose fellow was the enemy r\n",
            "epoch: 21, loss: 1.4544, accuracy: 69.07, LR: 0.00177059\n",
            "epoch: 22, loss: 1.4232, accuracy: 70.47, LR: 0.00177059\n",
            "epoch: 23, loss: 1.4304, accuracy: 70.47, LR: 0.00177059\n",
            "epoch: 24, loss: 1.4226, accuracy: 70.28, LR: 0.00177059\n",
            "epoch: 25, loss: 1.3988, accuracy: 69.83, LR: 0.00177059\n",
            "epoch: 26, loss: 1.3631, accuracy: 71.43, LR: 0.00171747\n",
            "epoch: 27, loss: 1.3534, accuracy: 72.13, LR: 0.00171747\n",
            "epoch: 28, loss: 1.3497, accuracy: 71.94, LR: 0.00171747\n",
            "epoch: 29, loss: 1.3651, accuracy: 71.36, LR: 0.00171747\n",
            "epoch: 30, loss: 1.2945, accuracy: 71.94, LR: 0.00171747\n",
            "xt it each wonder into my daughter\n",
            "And happy lightnings, with my son and lours\n",
            "And habsolous earl of our own proudsing.\n",
            "\n",
            "ANTONIO:\n",
            "The fresh professing forth, I pray thee, farther.\n",
            "\n",
            "ANTONIO:\n",
            "The fresh p\n",
            "epoch: 31, loss: 1.3132, accuracy: 71.17, LR: 0.00166594\n",
            "epoch: 32, loss: 1.2855, accuracy: 73.66, LR: 0.00166594\n",
            "epoch: 33, loss: 1.2931, accuracy: 73.09, LR: 0.00166594\n",
            "epoch: 34, loss: 1.2185, accuracy: 73.66, LR: 0.00166594\n",
            "epoch: 35, loss: 1.2139, accuracy: 74.36, LR: 0.00166594\n",
            "epoch: 36, loss: 1.1938, accuracy: 74.11, LR: 0.00161597\n",
            "epoch: 37, loss: 1.1483, accuracy: 75.26, LR: 0.00161597\n",
            "epoch: 38, loss: 1.1720, accuracy: 74.23, LR: 0.00161597\n",
            "epoch: 39, loss: 1.1658, accuracy: 74.62, LR: 0.00161597\n",
            "epoch: 40, loss: 1.1798, accuracy: 74.94, LR: 0.00161597\n",
            "and sittining in the seat o' the state,\n",
            "To whom the world all then. For man, what a spirit hath married them?\n",
            "\n",
            "MIRANDA:\n",
            "Sir, he may live:\n",
            "I saw him brother is my father said thee from the world.\n",
            "\n",
            "ANTON\n",
            "epoch: 41, loss: 1.1459, accuracy: 75.32, LR: 0.00156749\n",
            "epoch: 42, loss: 1.1241, accuracy: 76.21, LR: 0.00156749\n",
            "epoch: 43, loss: 1.1255, accuracy: 76.59, LR: 0.00156749\n",
            "epoch: 44, loss: 1.1022, accuracy: 77.17, LR: 0.00156749\n",
            "epoch: 45, loss: 1.1415, accuracy: 75.06, LR: 0.00156749\n",
            "epoch: 46, loss: 1.1157, accuracy: 76.40, LR: 0.00152046\n",
            "epoch: 47, loss: 1.1268, accuracy: 75.70, LR: 0.00152046\n",
            "epoch: 48, loss: 1.1109, accuracy: 74.87, LR: 0.00152046\n",
            "epoch: 49, loss: 1.0699, accuracy: 77.61, LR: 0.00152046\n",
            "epoch: 50, loss: 1.0861, accuracy: 77.68, LR: 0.00152046\n",
            "t to me;\n",
            "And I end spring them their sufferance follow'd.\n",
            "Where then will not call me Katharina!\n",
            "\n",
            "First Senator:\n",
            "Thou fatsm'st all under ang at the warm with so for as\n",
            "The war that spoken like as angre\n",
            "epoch: 51, loss: 1.0854, accuracy: 77.42, LR: 0.00147485\n",
            "epoch: 52, loss: 1.0455, accuracy: 78.06, LR: 0.00147485\n",
            "epoch: 53, loss: 1.0532, accuracy: 77.74, LR: 0.00147485\n",
            "epoch: 54, loss: 1.0145, accuracy: 78.95, LR: 0.00147485\n",
            "epoch: 55, loss: 0.9955, accuracy: 78.76, LR: 0.00147485\n",
            "epoch: 56, loss: 0.9723, accuracy: 79.40, LR: 0.00143060\n",
            "epoch: 57, loss: 0.9738, accuracy: 80.04, LR: 0.00143060\n",
            "epoch: 58, loss: 0.9767, accuracy: 79.53, LR: 0.00143060\n",
            "epoch: 59, loss: 1.0123, accuracy: 78.83, LR: 0.00143060\n",
            "epoch: 60, loss: 1.0064, accuracy: 78.89, LR: 0.00143060\n",
            "the dukedom and impleion\n",
            "At ever the devil have prevail'd like.\n",
            "\n",
            "SEBASTIAN:\n",
            "O, the fairnest nor of me.\n",
            "\n",
            "ANTONIO:\n",
            "What say'st thou, sir? O counterfeit stands, we will confess\n",
            "To thee unto my master.\n",
            "\n",
            "PR\n",
            "epoch: 61, loss: 0.8859, accuracy: 81.95, LR: 0.00138769\n",
            "epoch: 62, loss: 0.9490, accuracy: 79.59, LR: 0.00138769\n",
            "epoch: 63, loss: 0.9605, accuracy: 79.66, LR: 0.00138769\n",
            "epoch: 64, loss: 0.8974, accuracy: 81.06, LR: 0.00138769\n",
            "epoch: 65, loss: 0.8993, accuracy: 80.17, LR: 0.00138769\n",
            "epoch: 66, loss: 0.8983, accuracy: 80.61, LR: 0.00134605\n",
            "epoch: 67, loss: 0.8508, accuracy: 81.70, LR: 0.00134605\n",
            "epoch: 68, loss: 0.8628, accuracy: 81.76, LR: 0.00134605\n",
            "epoch: 69, loss: 0.8662, accuracy: 81.38, LR: 0.00134605\n",
            "epoch: 70, loss: 0.8909, accuracy: 80.80, LR: 0.00134605\n",
            "That thou shouldst prove suppiritune\n",
            "By every garden pieced agains him bide away thee women\n",
            "To all remember'd. I am married the lie; darest not the king's ship\n",
            "The maning swifter stone have left them f\n",
            "epoch: 71, loss: 0.8472, accuracy: 82.33, LR: 0.00130567\n",
            "epoch: 72, loss: 0.8672, accuracy: 81.51, LR: 0.00130567\n",
            "epoch: 73, loss: 0.8560, accuracy: 81.70, LR: 0.00130567\n",
            "epoch: 74, loss: 0.8224, accuracy: 83.55, LR: 0.00130567\n",
            "epoch: 75, loss: 0.8331, accuracy: 81.82, LR: 0.00130567\n",
            "epoch: 76, loss: 0.8290, accuracy: 81.38, LR: 0.00126650\n",
            "epoch: 77, loss: 0.8006, accuracy: 82.65, LR: 0.00126650\n",
            "epoch: 78, loss: 0.8245, accuracy: 83.42, LR: 0.00126650\n",
            "epoch: 79, loss: 0.7955, accuracy: 83.48, LR: 0.00126650\n",
            "epoch: 80, loss: 0.7638, accuracy: 83.48, LR: 0.00126650\n",
            "And paid which I have fow'd so far\n",
            "That I am mad it boundly on a sud, sorrow\n",
            "Say not too long, I am conceiving stars\n",
            "And perish on the penitent of our sea world say\n",
            "'Th say good husbandry.\n",
            "\n",
            "VIRCINTIO:\n",
            "\n",
            "epoch: 81, loss: 0.7656, accuracy: 84.63, LR: 0.00122851\n",
            "epoch: 82, loss: 0.7462, accuracy: 83.93, LR: 0.00122851\n",
            "epoch: 83, loss: 0.7433, accuracy: 84.12, LR: 0.00122851\n",
            "epoch: 84, loss: 0.7878, accuracy: 82.91, LR: 0.00122851\n",
            "epoch: 85, loss: 0.7824, accuracy: 83.86, LR: 0.00122851\n",
            "epoch: 86, loss: 0.7429, accuracy: 84.31, LR: 0.00119165\n",
            "epoch: 87, loss: 0.7657, accuracy: 83.80, LR: 0.00119165\n",
            "epoch: 88, loss: 0.7300, accuracy: 84.57, LR: 0.00119165\n",
            "epoch: 89, loss: 0.7201, accuracy: 85.33, LR: 0.00119165\n",
            "epoch: 90, loss: 0.7598, accuracy: 83.80, LR: 0.00119165\n",
            "ke a fresh of me,\n",
            "Murtern and long to him and his traniors\n",
            "Or bid my swiringar presently to the place,\n",
            "And this supper-sparit in your cabinion\n",
            "But one pursue most ascelbity!\n",
            "Of good coming tom, look th\n",
            "epoch: 91, loss: 0.7011, accuracy: 86.03, LR: 0.00115590\n",
            "epoch: 92, loss: 0.7315, accuracy: 84.57, LR: 0.00115590\n",
            "epoch: 93, loss: 0.7545, accuracy: 84.50, LR: 0.00115590\n",
            "epoch: 94, loss: 0.7282, accuracy: 84.76, LR: 0.00115590\n",
            "epoch: 95, loss: 0.6952, accuracy: 85.65, LR: 0.00115590\n",
            "epoch: 96, loss: 0.6987, accuracy: 86.16, LR: 0.00112123\n",
            "epoch: 97, loss: 0.6907, accuracy: 86.03, LR: 0.00112123\n",
            "epoch: 98, loss: 0.7346, accuracy: 84.38, LR: 0.00112123\n",
            "epoch: 99, loss: 0.6678, accuracy: 85.84, LR: 0.00112123\n",
            "epoch: 100, loss: 0.6863, accuracy: 86.10, LR: 0.00112123\n",
            "xt too much strongusts\n",
            "This is a shift respected woman to the king's forth,\n",
            "To this most dangerous soldier there and fortune.\n",
            "\n",
            "ANTONIO:\n",
            "If she would concount a sight on honour\n",
            "Of the moon, why, howinki\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKiwQWkBlq1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}